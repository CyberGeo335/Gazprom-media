{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-03T15:49:36.645625Z",
     "start_time": "2024-06-03T15:49:31.954171Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from tqdm import tqdm\n",
    "from PIL import ImageFile"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T15:49:36.707628Z",
     "start_time": "2024-06-03T15:49:36.646628Z"
    }
   },
   "cell_type": "code",
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "DEVICE"
   ],
   "id": "341afe5e8978a9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T15:50:12.793432Z",
     "start_time": "2024-06-03T15:50:12.782432Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Путь к данным animals\n",
    "train_path = 'D:\\\\ProgPrj\\\\dsProjects\\\\gazprom-media\\\\ml\\\\dataset_kaggle\\\\food-101\\\\images'\n",
    "#test_path = 'D:\\\\ProgPrj\\\\dsProjects\\\\urfo_hack_deer\\\\DatasetForCLIP\\\\Val'"
   ],
   "id": "49497f60ddd047f",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T15:50:21.865628Z",
     "start_time": "2024-06-03T15:50:21.846615Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Преобразования для тренировочного и валидационного наборов данных\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.RandomApply(torch.nn.ModuleList([transforms.ColorJitter()]), p=0.25),\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.RandomRotation(degrees=(-10, 10)),\n",
    "    transforms.RandomGrayscale(p=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "    transforms.RandomErasing(p=0.1, value='random')\n",
    "])"
   ],
   "id": "f32f91ad47a569b",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T15:50:27.997751Z",
     "start_time": "2024-06-03T15:50:27.990752Z"
    }
   },
   "cell_type": "code",
   "source": [
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])"
   ],
   "id": "df8beccbf2c93eeb",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T15:50:34.239358Z",
     "start_time": "2024-06-03T15:50:33.970476Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Загрузка данных\n",
    "train_dataset = datasets.ImageFolder(train_path, transform=train_transform)\n",
    "#val_dataset = datasets.ImageFolder(test_path, transform=val_transform)"
   ],
   "id": "711b3a5d5527813c",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T15:50:47.831218Z",
     "start_time": "2024-06-03T15:50:47.828218Z"
    }
   },
   "cell_type": "code",
   "source": "print(len(train_dataset.class_to_idx))",
   "id": "6ae96f2d7f5197ce",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T15:51:00.875359Z",
     "start_time": "2024-06-03T15:51:00.871356Z"
    }
   },
   "cell_type": "code",
   "source": [
    "batch_size = 64\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "#val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)"
   ],
   "id": "9081267386426ba2",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T15:51:09.229464Z",
     "start_time": "2024-06-03T15:51:03.903817Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Загрузка модели и процессора CLIP\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")"
   ],
   "id": "a39167356fdf9ac1",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Geo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Geo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T15:51:17.715178Z",
     "start_time": "2024-06-03T15:51:17.711181Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Размерность выходных признаков из модели CLIP\n",
    "hidden_size = model.config.projection_dim"
   ],
   "id": "194681d3fad7d3aa",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T15:51:26.208543Z",
     "start_time": "2024-06-03T15:51:26.200473Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Добавляем новый классификационный слой\n",
    "class CustomCLIPModel(nn.Module):\n",
    "    def __init__(self, clip_model, num_classes):\n",
    "        super(CustomCLIPModel, self).__init__()\n",
    "        self.clip_model = clip_model\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        with torch.no_grad():\n",
    "            features = self.clip_model.get_image_features(x)\n",
    "        x = self.fc(features)\n",
    "        return x"
   ],
   "id": "5e69aa7dccd04739",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T15:51:33.524602Z",
     "start_time": "2024-06-03T15:51:33.311603Z"
    }
   },
   "cell_type": "code",
   "source": [
    "num_classes = len(train_dataset.classes)\n",
    "custom_model = CustomCLIPModel(model, num_classes).to(DEVICE)"
   ],
   "id": "b4ef21f09be2e047",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T15:51:44.221744Z",
     "start_time": "2024-06-03T15:51:44.206421Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Оптимизатор и функция потерь\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(custom_model.parameters(), lr=0.0005)"
   ],
   "id": "a20de7072b03745c",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T15:51:54.861576Z",
     "start_time": "2024-06-03T15:51:54.847559Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Обучение с использованием градиентного скейлера\n",
    "from torch.cuda.amp import GradScaler, autocast"
   ],
   "id": "f1661489d6182dd3",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T15:51:59.338100Z",
     "start_time": "2024-06-03T15:51:59.328654Z"
    }
   },
   "cell_type": "code",
   "source": [
    "num_epochs = 50\n",
    "train_loss_history = []\n",
    "val_accuracy_history = []\n",
    "val_f1_history = []\n",
    "\n",
    "scaler = GradScaler()"
   ],
   "id": "91e2d9983a8b0fdb",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T16:11:34.753423Z",
     "start_time": "2024-06-03T15:52:04.458290Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for epoch in range(num_epochs):\n",
    "    ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "    custom_model.train()\n",
    "    running_loss = 0.0\n",
    "    train_loader_tqdm = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}')\n",
    "\n",
    "    for inputs, labels in train_loader_tqdm:\n",
    "        inputs, labels = inputs.to(DEVICE, non_blocking=True), labels.to(DEVICE, non_blocking=True)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with autocast():\n",
    "            outputs = custom_model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        train_loader_tqdm.set_postfix(loss=loss.item())\n",
    "\n",
    "    epoch_loss = running_loss / len(train_dataset)\n",
    "    train_loss_history.append(epoch_loss)"
   ],
   "id": "2dac92f8bc9fe977",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50: 100%|██████████| 1579/1579 [01:54<00:00, 13.77it/s, loss=1.7] \n",
      "Epoch 2/50: 100%|██████████| 1579/1579 [02:01<00:00, 12.97it/s, loss=0.499]\n",
      "Epoch 3/50: 100%|██████████| 1579/1579 [01:53<00:00, 13.91it/s, loss=0.925]\n",
      "Epoch 4/50: 100%|██████████| 1579/1579 [01:52<00:00, 14.02it/s, loss=0.953]\n",
      "Epoch 5/50: 100%|██████████| 1579/1579 [01:56<00:00, 13.56it/s, loss=1.22] \n",
      "Epoch 6/50: 100%|██████████| 1579/1579 [01:52<00:00, 14.04it/s, loss=0.558]\n",
      "Epoch 7/50: 100%|██████████| 1579/1579 [01:54<00:00, 13.80it/s, loss=1.2]  \n",
      "Epoch 8/50: 100%|██████████| 1579/1579 [01:54<00:00, 13.73it/s, loss=1.03] \n",
      "Epoch 9/50: 100%|██████████| 1579/1579 [01:56<00:00, 13.52it/s, loss=0.808]\n",
      "Epoch 10/50: 100%|██████████| 1579/1579 [01:56<00:00, 13.54it/s, loss=1.57] \n",
      "Epoch 11/50:   6%|▌         | 92/1579 [00:15<04:09,  5.95it/s, loss=1.03]  \n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "Caught FileNotFoundError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"C:\\Users\\Geo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"C:\\Users\\Geo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 51, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"C:\\Users\\Geo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 51, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"C:\\Users\\Geo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\datasets\\folder.py\", line 229, in __getitem__\n    sample = self.loader(path)\n  File \"C:\\Users\\Geo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\datasets\\folder.py\", line 268, in default_loader\n    return pil_loader(path)\n  File \"C:\\Users\\Geo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\datasets\\folder.py\", line 246, in pil_loader\n    with open(path, \"rb\") as f:\nFileNotFoundError: [Errno 2] No such file or directory: 'D:\\\\ProgPrj\\\\dsProjects\\\\gazprom-media\\\\ml\\\\dataset_kaggle\\\\food-101\\\\images\\\\apple_pie\\\\3501006.jpg'\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[16], line 7\u001B[0m\n\u001B[0;32m      4\u001B[0m running_loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.0\u001B[39m\n\u001B[0;32m      5\u001B[0m train_loader_tqdm \u001B[38;5;241m=\u001B[39m tqdm(train_loader, desc\u001B[38;5;241m=\u001B[39m\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mEpoch \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mnum_epochs\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m----> 7\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m inputs, labels \u001B[38;5;129;01min\u001B[39;00m train_loader_tqdm:\n\u001B[0;32m      8\u001B[0m     inputs, labels \u001B[38;5;241m=\u001B[39m inputs\u001B[38;5;241m.\u001B[39mto(DEVICE, non_blocking\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m), labels\u001B[38;5;241m.\u001B[39mto(DEVICE, non_blocking\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m     10\u001B[0m     optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\std.py:1181\u001B[0m, in \u001B[0;36mtqdm.__iter__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1178\u001B[0m time \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_time\n\u001B[0;32m   1180\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 1181\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m obj \u001B[38;5;129;01min\u001B[39;00m iterable:\n\u001B[0;32m   1182\u001B[0m         \u001B[38;5;28;01myield\u001B[39;00m obj\n\u001B[0;32m   1183\u001B[0m         \u001B[38;5;66;03m# Update and possibly print the progressbar.\u001B[39;00m\n\u001B[0;32m   1184\u001B[0m         \u001B[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    627\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    628\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[0;32m    629\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[1;32m--> 630\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    631\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m    632\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    633\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    634\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1345\u001B[0m, in \u001B[0;36m_MultiProcessingDataLoaderIter._next_data\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1343\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   1344\u001B[0m     \u001B[38;5;28;01mdel\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_task_info[idx]\n\u001B[1;32m-> 1345\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_process_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1371\u001B[0m, in \u001B[0;36m_MultiProcessingDataLoaderIter._process_data\u001B[1;34m(self, data)\u001B[0m\n\u001B[0;32m   1369\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_try_put_index()\n\u001B[0;32m   1370\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data, ExceptionWrapper):\n\u001B[1;32m-> 1371\u001B[0m     \u001B[43mdata\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreraise\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1372\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m data\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\_utils.py:694\u001B[0m, in \u001B[0;36mExceptionWrapper.reraise\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    690\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:\n\u001B[0;32m    691\u001B[0m     \u001B[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001B[39;00m\n\u001B[0;32m    692\u001B[0m     \u001B[38;5;66;03m# instantiate since we don't know how to\u001B[39;00m\n\u001B[0;32m    693\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(msg) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m--> 694\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m exception\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: Caught FileNotFoundError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"C:\\Users\\Geo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"C:\\Users\\Geo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 51, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"C:\\Users\\Geo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 51, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"C:\\Users\\Geo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\datasets\\folder.py\", line 229, in __getitem__\n    sample = self.loader(path)\n  File \"C:\\Users\\Geo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\datasets\\folder.py\", line 268, in default_loader\n    return pil_loader(path)\n  File \"C:\\Users\\Geo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\datasets\\folder.py\", line 246, in pil_loader\n    with open(path, \"rb\") as f:\nFileNotFoundError: [Errno 2] No such file or directory: 'D:\\\\ProgPrj\\\\dsProjects\\\\gazprom-media\\\\ml\\\\dataset_kaggle\\\\food-101\\\\images\\\\apple_pie\\\\3501006.jpg'\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "fcf8c43e6bb157ba"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
